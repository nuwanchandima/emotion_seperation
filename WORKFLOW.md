# Visual Workflow Guide

## ğŸ¬ Complete Pipeline Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  START: You have a video file (MP4, AVI, MOV, etc.)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Run Setup Check                                        â”‚
â”‚  $ python setup_check.py                                        â”‚
â”‚                                                                 â”‚
â”‚  Verifies:                                                      â”‚
â”‚  â€¢ Python 3.8+      âœ“                                          â”‚
â”‚  â€¢ FFmpeg           âœ“                                          â”‚
â”‚  â€¢ Dependencies     âœ“                                          â”‚
â”‚  â€¢ Directory setup  âœ“                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Run Tests (Optional but Recommended)                  â”‚
â”‚  $ python test_pipeline.py                                      â”‚
â”‚                                                                 â”‚
â”‚  Tests all components:                                          â”‚
â”‚  â€¢ Imports          âœ“                                          â”‚
â”‚  â€¢ Configuration    âœ“                                          â”‚
â”‚  â€¢ Face detection   âœ“                                          â”‚
â”‚  â€¢ Utilities        âœ“                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Run Pipeline                                           â”‚
â”‚  $ python src/pipeline.py your_video.mp4                        â”‚
â”‚                                                                 â”‚
â”‚  Or with options:                                               â”‚
â”‚  $ python src/pipeline.py video.mp4 --output results/          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Pipeline Stages  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘                                         â•‘
        â–¼                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Video Track   â”‚                         â”‚ Audio Track   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                         â”‚
        â”‚ [1] Extract Media                       â”‚
        â”‚     â€¢ FFmpeg audio extraction           â”‚
        â”‚     â€¢ Frame generator setup             â”‚
        â”‚                                         â”‚
        â”‚     âœ“ data/audio.wav                    â”‚
        â”‚                                         â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚                                         â”‚
        â”‚ [2] Face Detection â†’ Tracking           â”‚
        â”‚     â€¢ RetinaFace/YOLO/OpenCV            â”‚
        â”‚     â€¢ BYTETrack multi-object tracking   â”‚
        â”‚     â€¢ FaceNet embeddings                â”‚
        â”‚     â€¢ Agglomerative clustering          â”‚
        â”‚                                         â”‚
        â”‚     âœ“ outputs/tracks_faces.json         â”‚
        â”‚                                         â”‚
        â”‚                           [3] Speaker Diarization
        â”‚                               â€¢ pyannote.audio
        â”‚                               â€¢ VAD + clustering
        â”‚                               â€¢ Overlap detection
        â”‚                                         â”‚
        â”‚                               âœ“ outputs/diarization.json
        â”‚                                         â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚                                         â”‚
        â”‚ [4] Active Speaker Detection            â”‚
        â”‚     â€¢ Lip motion extraction             â”‚
        â”‚     â€¢ Audio energy computation          â”‚
        â”‚     â€¢ Cross-correlation sync            â”‚
        â”‚                                         â”‚
        â”‚     âœ“ outputs/active_speaker.json       â”‚
        â”‚                                         â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚                                         â”‚
        â”‚ [5] A/V Matching (Hungarian Algorithm)  â”‚
        â”‚     â€¢ Build cost matrix                 â”‚
        â”‚       - Temporal overlap (40%)          â”‚
        â”‚       - ASD sync score (60%)            â”‚
        â”‚     â€¢ Linear assignment                 â”‚
        â”‚     â€¢ Off-screen detection              â”‚
        â”‚                                         â”‚
        â”‚     âœ“ outputs/av_map.json               â”‚
        â”‚                                         â”‚
        â”‚                           [6] Emotion Change Detection
        â”‚                               â€¢ Speech Emotion Recognition
        â”‚                               â€¢ MFCC + pitch + energy
        â”‚                               â€¢ wav2vec2 (optional)
        â”‚                               â€¢ PELT change-point
        â”‚                                         â”‚
        â”‚                               âœ“ outputs/emotion_changes.json
        â”‚                                         â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚                                         â”‚
        â”‚ [7] Clip Extraction                     â”‚
        â”‚     For each emotion change:            â”‚
        â”‚     â€¢ t_start = change_time - 0.5s      â”‚
        â”‚     â€¢ duration = 1.0s                   â”‚
        â”‚     â€¢ FFmpeg clip extraction            â”‚
        â”‚                                         â”‚
        â”‚     âœ“ outputs/clips/*.mp4               â”‚
        â”‚     âœ“ outputs/clips_manifest.md         â”‚
        â”‚                                         â”‚
        â–¼                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMPLETE! Pipeline finished successfully                       â”‚
â”‚                                                                 â”‚
â”‚  Generated outputs:                                             â”‚
â”‚  â€¢ Person IDs (person_1, person_2, ...)                        â”‚
â”‚  â€¢ Speaker IDs (SPEAKER_00, SPEAKER_01, ...)                   â”‚
â”‚  â€¢ A/V mappings with confidence scores                          â”‚
â”‚  â€¢ Emotion change timestamps                                    â”‚
â”‚  â€¢ Video clips at each change point                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Review Results                                         â”‚
â”‚                                                                 â”‚
â”‚  1. Open clips_manifest.md                                      â”‚
â”‚     â€¢ Human-readable summary                                    â”‚
â”‚     â€¢ All clips organized by person/speaker                     â”‚
â”‚                                                                 â”‚
â”‚  2. Watch clips in outputs/clips/                              â”‚
â”‚     â€¢ Each clip shows emotion transition                        â”‚
â”‚     â€¢ Named with emotion labels                                â”‚
â”‚                                                                 â”‚
â”‚  3. Examine JSON files                                          â”‚
â”‚     â€¢ tracks_faces.json - person tracks                        â”‚
â”‚     â€¢ av_map.json - personâ†”speaker links                       â”‚
â”‚     â€¢ emotion_changes.json - all changes                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Analyze & Use Results                                 â”‚
â”‚                                                                 â”‚
â”‚  Option A: Manual Review                                        â”‚
â”‚  â€¢ Watch clips in video player                                 â”‚
â”‚  â€¢ Read clips_manifest.md                                      â”‚
â”‚                                                                 â”‚
â”‚  Option B: Programmatic Analysis                               â”‚
â”‚  â€¢ Load JSON files in Python                                   â”‚
â”‚  â€¢ Build visualizations                                        â”‚
â”‚  â€¢ Export to CSV/database                                      â”‚
â”‚  â€¢ See example_usage.py for patterns                           â”‚
â”‚                                                                 â”‚
â”‚  Option C: Further Processing                                   â”‚
â”‚  â€¢ Feed clips to another model                                 â”‚
â”‚  â€¢ Combine with other data sources                             â”‚
â”‚  â€¢ Build dashboards or reports                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Quick Reference Commands

### Initial Setup
```bash
# Create environment
python -m venv venv
venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Set HuggingFace token (for diarization)
$env:HF_TOKEN="your_token"  # Windows PowerShell
export HF_TOKEN="your_token"  # Linux/Mac

# Verify installation
python setup_check.py
python test_pipeline.py
```

### Running the Pipeline
```bash
# Basic usage
python src\pipeline.py video.mp4

# With custom output directory
python src\pipeline.py video.mp4 --output my_results\

# With custom config
python src\pipeline.py video.mp4 --config my_config.yaml
```

### Examining Results
```bash
# View clips manifest (human-readable)
type outputs\clips_manifest.md  # Windows
cat outputs/clips_manifest.md   # Linux/Mac

# List all clips
dir outputs\clips\  # Windows
ls outputs/clips/   # Linux/Mac

# Check logs
type outputs\pipeline.log  # Windows
tail -f outputs/pipeline.log  # Linux/Mac (live)

# Open in Python
python
>>> from src.utils import load_json
>>> results = load_json('outputs/emotion_changes.json')
>>> print(results)
```

### Running Individual Stages
```bash
# Stage 1: Extract media
python src\extract_media.py video.mp4

# Stage 2: Face tracking
python src\faces_track_cluster.py video.mp4

# Stage 3: Diarization
python src\diarize.py data\audio.wav

# Stage 4: Active speaker
python src\active_speaker.py video.mp4 data\audio.wav

# Stage 5: A/V matching
python src\av_match.py

# Stage 6: Emotion detection
python src\emotion_change.py data\audio.wav

# Stage 7: Clip extraction
python src\export_clips.py video.mp4
```

## ğŸ“Š Output Files Reference

### JSON Files (Machine-Readable)
| File | Contains | Size |
|------|----------|------|
| `tracks_faces.json` | Person IDs, face tracks, segments | ~100KB |
| `diarization.json` | Speaker IDs, speech segments | ~50KB |
| `active_speaker.json` | Lip-audio sync scores | ~200KB |
| `av_map.json` | Personâ†”Speaker mappings | ~10KB |
| `emotion_changes.json` | Emotion shifts with timestamps | ~30KB |
| `clips_summary.json` | Clip metadata | ~50KB |

### Other Files
| File | Purpose | Size |
|------|---------|------|
| `diarization.rttm` | Standard RTTM format for evaluation | ~20KB |
| `clips_manifest.md` | Human-readable clip index | ~20KB |
| `pipeline.log` | Detailed execution logs | ~1MB |
| `clips/*.mp4` | Video clips | ~1-5MB each |

## ğŸ”§ Configuration Quick Tweaks

### Make it Faster
```yaml
video:
  target_fps: 5  # â† Change from 10
face_detection:
  model: "opencv"  # â† Change from "retinaface"
clips:
  codec: "copy"  # â† Keep as "copy"
```

### Make it More Accurate
```yaml
video:
  target_fps: 15  # â† Change from 10
face_detection:
  confidence_threshold: 0.95  # â† Change from 0.9
emotion:
  model: "wav2vec2-ser"  # â† Keep transformer model
change_detection:
  penalty: 5  # â† Change from 10 (more sensitive)
```

### Reduce Memory Usage
```yaml
performance:
  batch_size: 8  # â† Change from 32
  cache_embeddings: false  # â† Change from true
video:
  target_fps: 5  # â† Lower frame rate
```

## ğŸ“ Understanding the Outputs

### Person vs Speaker
```
PERSON_1 (visual)  â†â†’  SPEAKER_00 (audio)
â€¢ Detected by face     â€¢ Detected by voice
â€¢ Tracked across       â€¢ Segmented by
  frames                 speech activity
â€¢ May appear/          â€¢ May be on or
  disappear              off screen
```

### Emotion Change Format
```json
{
  "t": 14.1,                    // Timestamp in seconds
  "from": {
    "label": "neutral",         // Discrete emotion
    "valence": 0.1,            // Pleasure (-1 to 1)
    "arousal": 0.0             // Energy (-1 to 1)
  },
  "to": {
    "label": "happy",
    "valence": 0.6,
    "arousal": 0.5
  }
}
```

### Clip Naming Convention
```
person_1_change_000_neutral_to_happy_t14.1s.mp4
â”‚       â”‚ â”‚      â”‚   â”‚       â”‚  â”‚     â”‚  â”‚   â”‚
â”‚       â”‚ â”‚      â”‚   â”‚       â”‚  â”‚     â”‚  â”‚   â””â”€ Extension
â”‚       â”‚ â”‚      â”‚   â”‚       â”‚  â”‚     â”‚  â””â”€â”€â”€â”€â”€ Timestamp
â”‚       â”‚ â”‚      â”‚   â”‚       â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€Time indicator
â”‚       â”‚ â”‚      â”‚   â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€To emotion
â”‚       â”‚ â”‚      â”‚   â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Transition
â”‚       â”‚ â”‚      â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€From emotion
â”‚       â”‚ â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Change index
â”‚       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"change" literal
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Person/Speaker ID
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Entity type
```

## ğŸš¨ Troubleshooting Decision Tree

```
Pipeline fails?
â”‚
â”œâ”€ During setup?
â”‚  â”œâ”€ "FFmpeg not found" â†’ Install FFmpeg, add to PATH
â”‚  â”œâ”€ "ImportError" â†’ pip install -r requirements.txt
â”‚  â””â”€ "Permission denied" â†’ Run as admin or change output dir
â”‚
â”œâ”€ During face detection?
â”‚  â”œâ”€ "No faces found" â†’ Lower confidence_threshold
â”‚  â”œâ”€ "CUDA out of memory" â†’ Reduce batch_size
â”‚  â””â”€ "Too slow" â†’ Use opencv model, reduce target_fps
â”‚
â”œâ”€ During diarization?
â”‚  â”œâ”€ "No audio stream" â†’ Check video has audio (ffprobe)
â”‚  â”œâ”€ "pyannote fails" â†’ Set HF_TOKEN environment variable
â”‚  â””â”€ "No speakers found" â†’ Check audio quality
â”‚
â””â”€ During emotion detection?
   â”œâ”€ "No changes found" â†’ Lower penalty in config
   â”œâ”€ "Too many changes" â†’ Increase penalty
   â””â”€ "Model download fails" â†’ Use "features" model
```

## âœ… Success Checklist

Before considering it "working":
- [ ] `setup_check.py` all green
- [ ] `test_pipeline.py` passes
- [ ] Pipeline runs without errors
- [ ] `outputs/clips/` contains video files
- [ ] `outputs/clips_manifest.md` readable
- [ ] Clips play correctly in video player
- [ ] Emotion labels make sense for content
- [ ] Logs show reasonable processing time

## ğŸ‰ You're Done!

Pipeline is working when you see:
```
âœ“ [7/7] Exporting emotion change clips (100% complete, 180s elapsed)
Pipeline complete! Total time: 180.3s (3.0 minutes)

Key results:
  - Persons detected: 2
  - Speakers detected: 2
  - A/V matches: 2
  - Emotion changes: 8
  - Clips exported: 8
```

Now go analyze some videos! ğŸ¬âœ¨
