# A/V Emotion Detection Pipeline

A production-ready system for **face tracking, speaker diarization, audio-visual mapping, and emotion change detection** in video content. Perfect for analyzing films, interviews, meetings, and any multi-speaker video content.

> **ğŸš¨ INSTALLATION ISSUE FIXED**: If you encountered `uv add -r requirements.txt` errors, see [`QUICK_FIX.md`](QUICK_FIX.md) for immediate solution!

## ğŸ¯ What You Get

1. **Person Roster**: Unique face IDs (`person_1`, `person_2`, ...) with persistent tracking across the video
2. **Speaker Roster**: Speaker diarization with overlapping speech detection (`speaker_1`, `speaker_2`, ...)
3. **A/V Mapping**: Best match between visible persons and speakers with confidence scores
4. **Emotion Changes**: Timestamps where vocal emotion shifts, plus auto-generated video clips

## ğŸ“š Documentation Quick Links

| Need | Read This | Time |
|------|-----------|------|
| ğŸš¨ **UV/pip error fix** | [`QUICK_FIX.md`](QUICK_FIX.md) | 30 sec |
| âœ… **Complete checklist** | [`CHECKLIST.md`](CHECKLIST.md) | 5 min |
| ğŸ”§ **Installation guide** | [`INSTALL.md`](INSTALL.md) | 10 min |
| âš¡ **Quick tutorial** | [`QUICKSTART.md`](QUICKSTART.md) | 5 min |
| ğŸ› **Troubleshooting** | [`TROUBLESHOOTING.md`](TROUBLESHOOTING.md) | As needed |
| ğŸ—ï¸ **Architecture** | [`ARCHITECTURE.md`](ARCHITECTURE.md) | 15 min |
| ğŸ“Š **Workflow** | [`WORKFLOW.md`](WORKFLOW.md) | 5 min |

## ğŸ—ï¸ Architecture

```
Input Video â†’ [Extract Media] â†’ Audio + Frames
                                     â†“
                          [Face Detection & Tracking]
                                     â†“
                          [Face Embedding & Clustering] â†’ Person IDs
                                     â†“
Audio â†’ [Speaker Diarization] â†’ Speaker IDs
                                     â†“
Audio + Faces â†’ [Active Speaker Detection] â†’ Lip-Audio Sync Scores
                                     â†“
                          [A/V Matching (Hungarian)] â†’ Person â†” Speaker Links
                                     â†“
Audio â†’ [Speech Emotion Recognition] â†’ Emotion Time Series
                                     â†“
                          [Change Point Detection] â†’ Emotion Shifts
                                     â†“
                          [Clip Extraction] â†’ Video Clips
```

## ğŸ“‹ Prerequisites

- **Python 3.8 - 3.12** (NOT 3.13+ due to package compatibility)
- **FFmpeg** (must be in PATH)
- **CUDA-capable GPU** (recommended for speed, but CPU works)

## ğŸš€ Quick Start

### 1. Install Dependencies

**Choose your installation method:**

#### Option A: UV (Fast, recommended for Linux/macOS)
```bash
uv venv --python 3.11
source .venv/bin/activate  # Linux/macOS
uv pip sync requirements.txt
```

#### Option B: PIP (Traditional, works everywhere)
```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS: source venv/bin/activate
                          # Windows: .\venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

ğŸ“– **Having issues?** See detailed instructions: [`INSTALL.md`](INSTALL.md)

**Note**: For pyannote.audio, you need to accept model terms and provide a HuggingFace token:
```bash
# 1. Visit https://huggingface.co/pyannote/speaker-diarization-3.1
# 2. Accept conditions and get your token from https://huggingface.co/settings/tokens
export HF_TOKEN=your_token_here
```

### 2. Run the Pipeline

```bash
# Full pipeline on a video
python src/pipeline.py path/to/your/video.mp4

# With custom config
python src/pipeline.py video.mp4 --config config.yaml

# Custom output directory
python src/pipeline.py video.mp4 --output results/my_analysis/
```

### 3. Check Results

All outputs go to `outputs/`:
- `tracks_faces.json` - Face tracks and person IDs
- `diarization.json` + `.rttm` - Speaker timeline
- `av_map.json` - Person â†” Speaker mappings
- `emotion_changes.json` - Emotion change timestamps
- `clips/` - Video clips around each change point
- `clips_manifest.md` - Human-readable clip index

## ğŸ“¦ Folder Structure

```
Task31_emotion_seperation/
â”œâ”€â”€ config.yaml              # Configuration (models, thresholds, etc.)
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ data/                   # Extracted media (audio.wav, etc.)
â”œâ”€â”€ outputs/                # Pipeline outputs
â”‚   â”œâ”€â”€ tracks_faces.json
â”‚   â”œâ”€â”€ diarization.json
â”‚   â”œâ”€â”€ av_map.json
â”‚   â”œâ”€â”€ emotion_changes.json
â”‚   â”œâ”€â”€ clips_summary.json
â”‚   â”œâ”€â”€ clips_manifest.md
â”‚   â””â”€â”€ clips/             # Emotion change video clips
â”œâ”€â”€ models/                # Downloaded model checkpoints (auto-created)
â””â”€â”€ src/
    â”œâ”€â”€ pipeline.py         # Main orchestrator
    â”œâ”€â”€ extract_media.py    # Audio/video extraction
    â”œâ”€â”€ faces_track_cluster.py  # Face detection â†’ tracking â†’ clustering
    â”œâ”€â”€ diarize.py          # Speaker diarization
    â”œâ”€â”€ active_speaker.py   # Lip-audio sync
    â”œâ”€â”€ av_match.py         # Hungarian matching
    â”œâ”€â”€ emotion_change.py   # SER + change-point detection
    â”œâ”€â”€ export_clips.py     # Clip extraction
    â””â”€â”€ utils.py            # Shared utilities
```

## ğŸ”§ Configuration

Edit `config.yaml` to customize:

- **Face Detection**: Model choice (RetinaFace, YOLOv8, MediaPipe), confidence thresholds
- **Face Tracking**: BYTETrack or DeepSORT settings
- **Face Clustering**: Agglomerative or DBSCAN with distance thresholds
- **Diarization**: pyannote.audio model, overlap detection
- **Active Speaker**: Window sizes, sync thresholds
- **Emotion Recognition**: Model (wav2vec2, ECAPA-TDNN), discrete vs continuous
- **Change Detection**: PELT, kernel CPD, penalty values
- **Clip Export**: Padding before/after, codec settings

## ğŸ¬ Example Outputs

### Face Tracking (`tracks_faces.json`)
```json
{
  "persons": [
    {
      "person_id": "person_1",
      "track_ids": [3, 17],
      "segments": [
        {"t0": 12.04, "t1": 28.20, "frames": 162}
      ],
      "embedding_mean": [0.12, -0.45, ...]
    }
  ]
}
```

### A/V Mapping (`av_map.json`)
```json
{
  "av_links": [
    {
      "person_id": "person_1",
      "speaker_id": "SPEAKER_00",
      "confidence": 0.87,
      "notes": "on-screen; strong lip-audio sync"
    },
    {
      "person_id": null,
      "speaker_id": "SPEAKER_02",
      "confidence": 0.74,
      "notes": "off-screen speaker"
    }
  ]
}
```

### Emotion Changes (`emotion_changes.json`)
```json
{
  "emotion_changes": {
    "SPEAKER_00": [
      {
        "t": 14.1,
        "from": {"label": "neutral", "valence": 0.1, "arousal": 0.0},
        "to": {"label": "happy", "valence": 0.6, "arousal": 0.5},
        "reason": "vocal emotion change"
      }
    ]
  }
}
```

## ğŸ¯ How It Handles Edge Cases

1. **Multiple people talking at once**: Diarization produces overlapping segments; ASD scores each visible face; Hungarian matching assigns optimally
2. **Visible person not talking**: Low ASD score â†’ no match â†’ that speaker maps to background/off-screen
3. **Off-screen speakers**: Diarized segments with no high ASD scores â†’ marked as `off_screen_speaker`
4. **Identity switches**: Embeddings + temporal smoothing re-associate tracks across scenes

## ğŸ§  Model Choices

| Component | Default Model | Alternatives |
|-----------|---------------|--------------|
| Face Detection | RetinaFace | YOLOv8-face, MediaPipe, OpenCV cascade |
| Face Tracking | BYTETrack | DeepSORT |
| Face Embedding | FaceNet | ArcFace, InsightFace |
| Diarization | pyannote/speaker-diarization-3.1 | SpeechBrain diarization |
| Emotion Recognition | wav2vec2-SER | ECAPA-TDNN, acoustic features |
| Change Detection | PELT (ruptures) | Kernel CPD, BottomUp |

## ğŸ“Š Performance Tips

- **Speed vs Accuracy**: Lower `target_fps` in config (e.g., 5-10 FPS for film analysis)
- **GPU Usage**: Set `use_gpu: true` and `mixed_precision: true` in config
- **Caching**: Enable `cache_embeddings: true` to speed up re-runs
- **Clip Codec**: Use `codec: copy` for fast extraction (slight timestamp imprecision) or `libx264` for precise cuts

## ğŸ› ï¸ Running Individual Stages

You can run pipeline stages independently:

```bash
# 1. Extract audio
python src/extract_media.py video.mp4

# 2. Face detection/tracking
python src/faces_track_cluster.py video.mp4

# 3. Diarization
python src/diarize.py data/audio.wav

# 4. Active speaker detection
python src/active_speaker.py video.mp4 data/audio.wav

# 5. A/V matching
python src/av_match.py

# 6. Emotion change detection
python src/emotion_change.py data/audio.wav

# 7. Export clips
python src/export_clips.py video.mp4
```

## ğŸ§ª Testing

```bash
# Test with a short sample video
python src/pipeline.py sample_video.mp4 --output test_output/

# Check logs
cat outputs/pipeline.log

# Review clips
ls outputs/clips/
```

## ğŸ“ Output Format Details

All JSON outputs use consistent timestamp units (seconds as floats). RTTM format is compatible with standard diarization evaluation tools (pyannote-metrics, NIST tools).

## ğŸ” Troubleshooting

**Issue**: `pyannote.audio` fails to load model
- **Solution**: Accept HuggingFace model terms, provide token via `HF_TOKEN` env var

**Issue**: Face detection is slow
- **Solution**: Reduce `target_fps`, use OpenCV cascade instead of RetinaFace

**Issue**: No audio stream found
- **Solution**: Check video has audio track with `ffprobe -i video.mp4`

**Issue**: Emotion changes not detected
- **Solution**: Lower `penalty` in change_detection config; check audio quality

**Issue**: Clips have wrong timestamps
- **Solution**: Use `codec: libx264` instead of `copy` for precise cuts

## ğŸ“š Citation & References

This pipeline integrates techniques from:
- **BYTETrack**: Zhang et al., "ByteTrack: Multi-Object Tracking by Associating Every Detection Box"
- **pyannote.audio**: Bredin et al., "pyannote.audio 2.1: speaker diarization"
- **SyncNet**: Chung & Zisserman, "Out of time: automated lip sync in the wild"
- **ruptures**: Truong et al., "Selective review of offline change point detection methods"

## ğŸ¤ Contributing

Improvements welcome! Key areas:
- Additional emotion models (facial expression, multimodal fusion)
- Better active speaker models (TalkNet, AVA-ActiveSpeaker)
- Scene boundary detection
- Speaker identification (match to known voices)

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ‰ Acknowledgments

Built with â¤ï¸ using PyTorch, OpenCV, librosa, pyannote.audio, and ruptures.

---

**Happy analyzing! ğŸ¬ğŸ”âœ¨**

For questions or issues, check the logs in `outputs/pipeline.log` or open an issue on GitHub.
